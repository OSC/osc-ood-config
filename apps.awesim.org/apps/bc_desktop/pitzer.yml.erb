<%-
  groups = OodSupport::User.new.groups.sort_by(&:id).tap { |groups|
    groups.unshift(groups.delete(OodSupport::Process.group))
  }.map(&:name).grep(/^P./)
-%>
---
title: "Pitzer Desktop"
cluster: "pitzer"
description: |
  This app will launch an interactive desktop on one or more compute nodes. It is
  a large environment for when you need a lot of compute and/or memory resources because 
  you will have full access to all the resources on that compute node(s).
  
  If you do not need all these resources, use the 
  [Lightweight Desktop](/pun/sys/dashboard/batch_connect/sys/bc_desktop/vdi/session_contexts/new)
  app instead which is much more lightweight for general-purpose use cases.
form:
  # everything is taken from bc_desktop/form.yml except cores is added
  - bc_vnc_idle
  - desktop
  - account
  - bc_num_hours
  - gpus
  - cores
  - bc_num_slots
  - licenses
  - node_type
  - bc_queue
  - bc_vnc_resolution
  - bc_email_on_started
attributes:
  desktop:
    widget: select
    label: "Desktop environment"
    options:
      - ["Xfce", "xfce"]
      - ["Mate", "mate"]
    help: |
      This will launch either the [Xfce] or [Mate] desktop environment on the
      [Pitzer cluster].

      [Xfce]: https://xfce.org/
      [Mate]: https://mate-desktop.org/
      [Pitzer cluster]: https://www.osc.edu/supercomputing/computing/pitzer
  bc_queue: null
  account:
    label: "Project"
    widget: select
    options:
      <%- groups.each do |group| %>
      - "<%= group %>"
      <%- end %>
  cores:
    widget: number_field
    value: 48
    min: 1
    max: 48
    step: 1
  gpus: ""
  licenses:
    help: |
      Licenses are comma separeated in the format '\<name\>@osc:\<# of licenses\>' like
      'stata@osc:1'. More help can be found on our website regarding the changes
      in the [slurm migration].

      [slurm migration]: https://www.osc.edu/resources/technical_support/supercomputers/owens/environment_changes_in_slurm_migration
    pattern: '^([\w]+@osc:\d+,{0,1})+$'
  node_type:
    widget: select
    label: "Node type"
    help: |
      - **Standard Compute** <br>
        These are standard HPC machines. There are 224 with 40 cores and
        340 with 48. They all have 192 GB of RAM. Chosing any will decrease
        your wait time.
      - **GPU Enabled** <br>
        These are HPC machines with [NVIDIA Tesla V100 GPUs]. They have the same
        40 core machines have 2 GPUs with 16 GB of RAM and 48 core machines have 2
        with 32 GB of RAM.  Densegpu types have 4 GPUs with 16 GB of RAM.
        Visualization nodes are GPU enabled nodes with an X Server in the background
        for 3D visualization using VirtualGL.
      - **Large Memory** <br>
        These are HPC machines with very large amounts of memory. Largmem nodes
        have 48 cores with 768 GB of RAM. Hugemem nodes have 80 cores with 3 TB of RAM.

      Visit the OSC site for more [detailed information on the Pitzer cluster].
      [detailed information on the Pitzer cluster]: https://www.osc.edu/resources/technical_support/supercomputers/pitzer
      [NVIDIA Tesla V100 GPUs]: https://www.nvidia.com/en-us/data-center/v100/
    options:
      - [ "any", "any" ]
      - [ "48 core", "any-48core" ]
      - [ "any gpu", "gpu-any"]
      - [ "48 core with gpu", "gpu-48core" ]
      - [ "densegpu", "densegpu" ]
      - [ "visualization node", "vis"]
      - [ "largemem", "largemem" ]
      - [ "hugemem", "hugemem" ]
submit: submit/slurm.yml.erb