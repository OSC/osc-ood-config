
<%- 
  base_slurm_args = ["--nodes", "#{bc_num_slots}"]
  base_slurm_args.concat ["--licenses", "#{licenses}"] unless licenses.empty?

  def tasks_per_node
    # if you request > 1 nodes, it doesn't matter how many cores you request - you get the whole node
    # but srun still adheres to how many tasks per node you've requested, not what you _have_.
    bc_num_slots.to_i > 1 ? [] : [ "--ntasks-per-node", "#{cores}" ]
  end

  def any_node
    tasks_per_node
  end

  def p18_node
    return tasks_per_node + [ "--constraint", "40core" ]
  end

  def p20_node
    return tasks_per_node + [ "--constraint", "48core" ]
  end

  def gpu_count
    if !gpus.nil? && !gpus.empty?
      gpus
    else
      1
    end
  end

  slurm_args = case node_type
              # 'any' case handled by scheduler, this is just a quick short circuit
              when "any"
                base_slurm_args + any_node

              when "any-40core"
                base_slurm_args + p18_node
              when "any-48core"
                base_slurm_args + p20_node

              when "gpu-any"
                base_slurm_args + any_node + ["--gpus-per-node", "#{gpu_count}"]
              when "gpu-40core"
                base_slurm_args + p18_node + ["--gpus-per-node", "#{gpu_count}"]
              when "gpu-48core"
                base_slurm_args + p20_node + ["--gpus-per-node", "#{gpu_count}"]
              when "vis"
                base_slurm_args + any_node + ["--gpus-per-node", "#{gpu_count}", "--gres", "vis"]
              when "densegpu"
                base_slurm_args + p20_node + ["--gpus-per-node", "4"]

              # using partitions here is easier than specifying memory requests
              when "largemem"
                partition = bc_num_slots.to_i > 1 ? "largemem-parallel" : "largemem"
                base_slurm_args + tasks_per_node + ["--partition", partition ]
              when "hugemem"
                partition = bc_num_slots.to_i > 1 ? "hugemem-parallel" : "hugemem"
                base_slurm_args + tasks_per_node + ["--partition", partition ]

              else
                base_slurm_args
              end

  image = '/apps/project/ondemand/singularity/mate-rhel8/mate-rhel8.sif'
-%>
---
batch_connect:
  before_script: |
    # Export the module function if it exists
    [[ $(type -t module) == "function"  ]] && export -f module

    # MATE acts strange in pitzer-exp and doesn't like /var/run/$(id -u)
    export XDG_RUNTIME_DIR="$TMPDIR/xdg_runtime"

    # reset SLURM_EXPORT_ENV so that things like srun & sbatch work out of the box
    export SLURM_EXPORT_ENV=ALL
  <%- if cluster == 'ascend' && desktop == 'mate' -%>
  script_wrapper: |
    cat << "CTRSCRIPT" > container.sh
    module load turbovnc/2.2.7 novnc/1.3.0
    %s  
    CTRSCRIPT

    # your bindpath will differ
    export APPTAINER_BINDPATH="/run,/apps,/tmp"

    apptainer run --nv <%= image %> /bin/bash container.sh
  <%- end -%>
script:
  accounting_id: "<%= account %>"
  native:
    <%- slurm_args.each do |arg| %>
    - "<%= arg %>"
    <%- end %>
