<%-
  groups = OodSupport::User.new.groups.sort_by(&:id).tap { |groups|
    groups.unshift(groups.delete(OodSupport::Process.group))
  }.map(&:name).grep(/^P./)
-%>
---
title: "Cardinal Desktop"
cluster: "cardinal"
description: |
  This app will launch an interactive desktop on one or more compute nodes. It is
  a large environment for when you need a lot of compute and/or memory resources because 
  you will have full access to all the resources on that compute node(s).
  
  If you do not need all these resources, use the 
  [Lightweight Desktop](/pun/sys/dashboard/batch_connect/sys/bc_desktop/vdi/session_contexts/new)
  app instead which is much more lightweight for general-purpose use cases.
form:
  # everything is taken from bc_desktop/form.yml except cores is added
  - bc_vnc_idle
  - desktop
  - account
  - bc_num_hours
  - gpus
  - cores
  - bc_num_slots
  - licenses
  - node_type
  - bc_queue
  - bc_vnc_resolution
  - bc_email_on_started
attributes:
  desktop:
    widget: select
    label: "Desktop environment"
    options:
      - ["Xfce", "xfce"]
      - ["Mate", "mate"]
    help: |
      This will launch either the [Xfce] or [Mate] desktop environment on the
      [Cardinal cluster].

      [Xfce]: https://xfce.org/
      [Mate]: https://mate-desktop.org/
      [Cardinal cluster]: https://www.osc.edu/supercomputing/computing/Cardinal
  bc_queue: null
  account:
    label: "Project"
    widget: select
    options:
      <%- groups.each do |group| %>
      - "<%= group %>"
      <%- end %>
  cores:
    widget: number_field
    value: 48
    min: 1
    max: 48
    step: 1
  gpus:
    widget: number_field
    min: 0
    max: 4
  licenses:
    help: |
      Licenses are comma separeated in the format '\<name\>@osc:\<# of licenses\>' like
      'stata@osc:1'. More help can be found on our website regarding the changes
      in the [slurm migration].

      [slurm migration]: https://www.osc.edu/resources/technical_support/supercomputers/owens/environment_changes_in_slurm_migration
    pattern: '^([\w]+@osc:\d+,{0,1})+$'
  node_type:
    widget: select
    label: "Node type"
    help: |
      - **Standard Compute** <br>
        These are standard HPC machines. There are 326 of these machines with
        96 cores. They all have 512 GB of RAM. Chosing any will decrease
        your wait time.
      - **GPU Enabled** <br>
        These are HPC machines with [NVIDIA Tesla H100 GPUs]. They have 96
        cores and  1 TB of RAM. 
      - **Visualization Nodes** <br>
        Visualization nodes are GPU enabled nodes with an X Server in the background
        for 3D visualization using VirtualGL.
      - **Huge Memory Nodes** <br>
        Huge Memory nodes have 2 TB of memory for memory intensive workloads.
        There are 16 of these machines on Cardinal.
      Visit the OSC site for more [detailed information on the Cardinal cluster].
      [detailed information on the Cardinal cluster]: https://www.osc.edu/resources/technical_support/supercomputers/cardinal
      [NVIDIA Tesla H100 GPUs]: https://www.nvidia.com/en-us/data-center/h100/
    options:
      - [
          "any", "any",
          data-min-cores: 1,
          data-max-cores: 96,
          data-set-gpus: 0,
        ]
      - [
          "any gpu", "gpu-any",
          data-min-cores: 1,
          data-max-cores: 96,
          data-set-gpus: 1,
        ]
      - [
        "visualization node", "vis",
        data-min-cores: 1,
        data-max-cores: 96,
        data-set-gpus: 1,
        ]
      - [
        "hugemem node", "hugemem",
        data-min-cores: 43,
        data-max-cores: 96,
        data-set-gpus: 1,
        ]
submit: submit/slurm.yml.erb