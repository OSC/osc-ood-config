
<%- 
  base_slurm_args = ["--nodes", "#{bc_num_slots}"]
  base_slurm_args.concat ["--licenses", "#{licenses}"] unless licenses.empty?

  def tasks_per_node
    [ "--ntasks-per-node", "#{cores}" ]
  end

  def any_node
    tasks_per_node
  end

  def p18_node
    return tasks_per_node + [ "--constraint", "40core" ]
  end

  def p20_node
    return tasks_per_node + [ "--constraint", "48core" ]
  end

  # gpu_count will always return at least 1, so take care when calling it.
  def gpu_count
    if !gpus.nil? && !gpus.empty? && gpus.to_i.positive?
      gpus
    else
      1
    end
  end

  # any* node types can possible get a gpu if they've set gpu >= 1
  def possible_gpus
    if gpus.to_s.to_i.positive?
      ["--gpus-per-node", "#{gpu_count}"]
    else
      []
    end
  end

  slurm_args = case node_type
              # 'any' case handled by scheduler, this is just a quick short circuit
              when "any"
                base_slurm_args + any_node + possible_gpus

              when "any-40core"
                base_slurm_args + p18_node + possible_gpus
              when "any-48core"
                base_slurm_args + p20_node + possible_gpus

              when "gpu-any"
                base_slurm_args + any_node + ["--gpus-per-node", "#{gpu_count}"]
              when "gpu-40core"
                base_slurm_args + p18_node + ["--gpus-per-node", "#{gpu_count}"]
              when "gpu-48core"
                base_slurm_args + p20_node + ["--gpus-per-node", "#{gpu_count}"]
              when "vis"
                base_slurm_args + any_node + ["--gpus-per-node", "#{gpu_count}", "--gres", "vis"]
              when "densegpu"
                base_slurm_args + p20_node + ["--gpus-per-node", "4"]

              # using partitions here is easier than specifying memory requests
              when "largemem"
                partition = bc_num_slots.to_i > 1 ? "largemem-parallel" : "largemem"
                base_slurm_args + tasks_per_node + ["--partition", partition ]
              when "hugemem"
                partition = bc_num_slots.to_i > 1 ? "hugemem-parallel" : "hugemem"
                base_slurm_args + tasks_per_node + ["--partition", partition ]

              else
                base_slurm_args
              end

  image = '/apps/project/ondemand/singularity/mate-rhel8/mate-rhel8.sif'
-%>
---
batch_connect:
  before_script: |
    # Export the module function if it exists
    [[ $(type -t module) == "function"  ]] && export -f module

    # MATE acts strange in pitzer-exp and doesn't like /var/run/$(id -u)
    export XDG_RUNTIME_DIR="$TMPDIR/xdg_runtime"

    # reset SLURM_EXPORT_ENV so that things like srun & sbatch work out of the box
    export SLURM_EXPORT_ENV=ALL
script:
  accounting_id: "<%= account %>"
  native:
    <%- slurm_args.each do |arg| %>
      - "<%= arg %>"
    <%- end %>
